# Raster, I present you Computer Graphics

The rasterization is one of the most efficient ways to solve visibility in compute graphics.
The idea spread really fast after vector monitors were replaced by raster monitors, the last 
with image buffers to cache the viewed image in form of an array of pixels and not a set of
lines.

Before raster graphics, 3D images where mainly wireframes with occluded surfaces hidden by
really expensive clipping algorithms.

Rasterization is the process of covering a portion of the screen with a specific logic, 
revealing those pixels that satisfies certain criteria, for instance be inside a triangle, be
part of a line, be a point. The final process turns continuous definitions into pixels.

An equivalent process is known as a raycaster, where the process casts a ray for every pixel
to find the closest geometry and render accordingly. Nevertheless, visibility on a rasterizer
is handled normally with a depth-buffer and this way, a coherence in space can be used to
efficiently recognize all pixels that belongs together to a primitive instead of repeating the 
check the order of the primitives in every pixel.

## The pipeline involving a rasterizer

**Vertex Process:** In order to make a rasterization process to render a 3D geometry properly a sequence of 
transformations are needed. Starting with the transformations of the vertices to be
projected on the normalized-device-coordinates space. All this transformations has to be 
managed inside a stage named **Vertex Shader**.

**Primitive Assembly and Clipping:** With vertices on this space, primitives are assembled forming triangles, lines or simply
stay as points. A necessary clip against the view-box is necessary to avoid
geometries, or part of geometries, that are not visible to write pixels in the render target.

Modern rasterizers clips only against z=0, and then clip the bounding box of the geometry 
against the box of the screen. Notice that clipping also may generate additional primitives, 
for instance, when a triangle corner is cut by a plane, then the quadrilateral left needs to
be expressed in terms of two triangles.

**De-homogenization and Viewport Scalling:** In this point all primitives still in 
normalized-device-coordinates, i.e., all positions are in homogenous space with 4 components.
The division with respect to $w$ is now possible since all values of $z \geq 0$ and therefore
$w \geq znear$.

**Rasterization:** With every primitive in front of the viewer, a rasterization process analyses
only the intersection between the projected box of the primitive and the screen. Modern hardware
process this potentially large area split into tiles of 8x8 or 16x16. Notice that if the triangle
is thin and diagonal, then the ratio of the coverage of the triangle with respect to the bounding
is small. On the other hand, parallelism can be better exploit if different tiles of the same
triangle can be processed in parallel. On the opposite case, large triangles in screen will 
require much more computation, wasting the time of processor units that finished small triangles
in the same group of threads and stayed idle. Tiling is not supported currently (volunteers?).

The rasterization of triangles, lines and points are quite different. Point case is not worthy 
to comment. Lines can be raster with mid-point technique or Bresenham algorithm. Triangles, on
the other hand, are raster with an inside test method. For this, barycentric coordinates in 
screen space are computed for the left-top-most pixel center. Barycentric coordinates varies 
from one pixel to another in a constant way! so is really fast to update the value and to check
if the pixel is in the interior or not. Also, automatically we can compute the real barycentric
coordinates of the triangle in real space (applying perspective correction) and voila! a 
fragment is generated by means of interpolation between the triangle vertices. A new fragment
to be processed.

**Fragment Process:** This is the final stage of the rasterization pipeline. First, data of 
the fragment (vertices interpolated) is processed to get an output that will be store in the 
render target. This output is commonly a color, but different techniques requires for the
rasterizer to solve other quantities like depth, position, normal. So, it does not need
to be always a color and neither always a single render target. For simplicity in `rendertoy`
we support a **fragment shader** that outputs a single `float4`.

After the color is compute a depth test is perform to check whenever the pixel must be visible
or not. If the pixel has lesser depth than the last pixel rendered, then the render target
is updated and also the depth buffer.

Now, let's check how all this is implemented in `rendertoy`. Luckily, most of this pipeline is
encapsulated on a `Raster` type that prepares necessary kernels to process geometries for a 
specific vertex type.

## Defining Vertex types

The first stage of the rasterizer is how to process vertices from a model into vertices that
has the information of the projected position and anything necessary for lighting. In our case
we will use the same definition of MeshVertex as input, so, only the output vertex is necessary
to declare.

```python
@ren.kernel_struct
class Vertex_Out:
    proj: ren.float4
    C: ren.float3
```
Our vertex output only needs to save the projected position `proj` and a computed per-vertex
color `C`. 

Attention here. Normally the vertex attribute that the rasterizer will use as projected position
must be indicated somehow (a special attribute, semantic, etc.). In our case we always will assume
the **first field** as the projected position.

Shading (coloring) in every vertex and then interpolating the color among pixels is known as 
Gouraud shading.

## Defining a vertex shader

In our rasterizer, a vertex shader is a `kernel_function` satisfying:
- Has only two arguments and a return type.
- Both arguments are struct types.
- The first argument will be assumed as the vertex to process.
- The second argument will be assumed as the global parameters required by the process.

For example, next code shows a possible vertex shader that receives the 3 transformations (world,
view and projection) and computes the projected position `H` and a lambert factor as color `C`. 

```python
# Create the buffer to store the matrices for the vertex shader
shader_globals = ren.create_struct(Transforms)

@ren.kernel_function  # Vertex shaders will be treated as kernel functions, the main function is implemented in the rasterizer
def transform_and_draw(
    vertex: ren.MeshVertex, info: Transforms
) -> Vertex_Out:
    """
    float3 P = vertex.P;
    float d = max(0.2f, dot(vertex.N, normalize((float3)(1,1,1))));
    float3 C = (float3)(d,d,d); // vertex.N * 0.5f + 0.5f; // use normals as a color for debugging purposes

    float4 H = (float4)(P.x, P.y, P.z, 1.0); // extend 3D position to a homogeneous coordinates

    H = mul(H, info.World); // transform with respect to world matrix
    H = mul(H, info.View);  // transform with respect to view matrix
    H = mul(H, info.Proj);  // transform with respect to projection matrix

    Vertex_Out o;
    o.proj = H;
    o.C = C;
    return o;
    """
```

## Fragment Shader

The `Raster` object hides all processes for primitives, clipping, viewport scalling.
For the time a fragment is generated (by means of interpolating triangle vertices) the
fragment shader is called.

The **fragment shader** is a `kernel_function` that satisfies:
- Has only two arguments and a return type.
- The first argument is the framgent and has to be the same type of the vertex output of the vertex shader in the same raster
- The second argument is a struct with necessary parameters for the processing.
- The return type must be `float4`.

In our case, because the vertex shader did process all the shading (coloring), we only
need to return such value as `float4`.

```python
@ren.kernel_function
def fragment_to_color(fragment: Vertex_Out, info: Transforms) -> ren.float4:
    """
    return (float4)(fragment.C, 1);
    """
```

## Creating a raster object

Once we have a vertex shader, a fragment shader and an image to render we can create a
raster object that performs all rasterizations using those objects.

```python
raster = ren.Raster(
    presenter.get_render_target(),  # render target to draw on
    transform_and_draw,             # vertex shader used, only transform and set normal as a color
    shader_globals,          # buffer with the transforms
    fragment_to_color,              # fragment shader, only return the color of the vertex
    shader_globals                            # unused buffer
)
```

Notice that the image is required and internally a depth-buffer is created with sufficient
space to have a depth float for each screen pixel.

The structs with the information for vertex shaders and fragment shaders needs to be passed here.
Important, that does not mean that those argument are fix. You can update their content between
drawing calls!  And you must!

Imaging you want to draw several objects. You define this raster but then between the draw call
of one object and another object you might change the world transform, or a color, or a material
property.

## Using the raster object

Important to clear the render target and the depth buffer in every frame.

```python
ren.clear(raster.get_render_target())
ren.clear(raster.get_depth_buffer(), 1.0)
raster.draw_triangles(vertex_buffer, index_buffer)
```

Notice the depth-buffer needs to be cleared with value $1$. The idea is that only fragments
with depth less than the one in the buffer are visible. The greatest value for the depths 
in the normalized-device-coordinates is 1.

The code here is exposed in the tutorial/lesson08_rasterization.py script.

